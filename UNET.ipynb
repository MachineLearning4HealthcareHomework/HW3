{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(36)\n",
    "\n",
    "\n",
    "#https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
    "import os\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, MaxPooling2D, GlobalMaxPool2D, Conv2D, Conv2DTranspose,concatenate, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the output (decision) to 3-class classifier with softmax activation and sparse categorical cross-entropy loss and reshape the output to vectorized pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
    "num_class = 3\n",
    "input_size = (256,256,1)\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size, batchnorm, activation):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "def get_unet(input_img, n_filters, dropout, batchnorm, kernel_size, activation, depth):\n",
    "    # Contracting Path\n",
    "    c=[None]*(depth+3)\n",
    "    #form 0(=init) to depth(=last decode), also bottom added as depth[depth+1]\n",
    "    p=[input_img]*(depth+2)\n",
    "    #Encode\n",
    "    for current_depth in range(depth):\n",
    "        current_depth+=1\n",
    "        c[current_depth]=conv2d_block(p[current_depth-1], n_filters * pow(2,current_depth-1), kernel_size, batchnorm, activation)\n",
    "        p[current_depth]=MaxPooling2D((2,2))(c[current_depth])\n",
    "        p[current_depth]=Dropout(dropout)(p[current_depth])\n",
    "    #Bottom\n",
    "    c[depth+1]=conv2d_block(p[depth],n_filters* pow(2,depth),kernel_size,batchnorm, activation)\n",
    "        \n",
    "    #Decode\n",
    "    u=[None]*(depth+2)\n",
    "    d=[c[depth+1]]*(depth+2)\n",
    "    for current_depth in reversed(range(depth)):\n",
    "        u[current_depth] = Conv2DTranspose(n_filters * pow(2,current_depth), (kernel_size, kernel_size), strides = (2, 2), padding = 'same')(d[current_depth+1])\n",
    "        u[current_depth] = concatenate([u[current_depth], c[current_depth+1]])\n",
    "        u[current_depth] = Dropout(dropout)(u[current_depth])\n",
    "        d[current_depth] = conv2d_block(u[current_depth], n_filters * pow(2,current_depth), kernel_size, batchnorm, activation)\n",
    "\n",
    "    #decision\n",
    "    outputs = Conv2D(num_class, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(d[0])\n",
    "    outputs = Conv2D(num_class, 1, activation = 'softmax')(outputs)\n",
    "    outputs = Reshape([input_size[0]*input_size[1],num_class])(outputs)\n",
    "    \n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jannik\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/U_net_16F25Dr3WsreA3D\\assets\n",
      "INFO:tensorflow:Assets written to: models/U_net_16F25Dr3WsreA5D\\assets\n",
      "INFO:tensorflow:Assets written to: models/U_net_32F25Dr3WsreA3D\\assets\n",
      "INFO:tensorflow:Assets written to: models/U_net_32F25Dr3WsreA5D\\assets\n"
     ]
    }
   ],
   "source": [
    "im_height = 256\n",
    "im_width  = 256\n",
    "\n",
    "input_img = Input((im_height, im_width, 1), name = 'img')\n",
    "\n",
    "# Hyperparameters\n",
    "filters = [16,32]\n",
    "#filters = [32]\n",
    "dropouts = [25]\n",
    "window_sizes = [3]\n",
    "activations = ['relu']\n",
    "depths = [3,5]\n",
    "#depthts= [7]\n",
    "for föuter in filters:\n",
    "    for dropout in dropouts:\n",
    "        for window_size in window_sizes:\n",
    "            for activation in activations:\n",
    "                for depth in depths:\n",
    "                    string = str(föuter)+'F'+str(dropout)+'Dr'+str(window_size)+'Ws'+activation[:2]+'A'+str(depth)+'D'\n",
    "\n",
    "                    # Get the UNET\n",
    "                    model = get_unet(input_img, föuter, dropout/100, True, window_size, activation, depth) \n",
    "\n",
    "                    # Compile the UNET\n",
    "                    model.compile(optimizer = Adam(), loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "                    plot_model(model, to_file='diagrams/'+string+'.png')\n",
    "                    path='models/U_net_' + string\n",
    "                    if not os.path.exists(path):\n",
    "                        os.makedirs(path)\n",
    "                    model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
